{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7404d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be in seperate file\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class RatingsTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ratings, all_product_ids):\n",
    "        self.users, self.items, self.labels = self.get_dataset(ratings, all_product_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "    def get_dataset(self, ratings, all_product_ids):\n",
    "        users, items, labels = [], [], []\n",
    "        user_item_set = set(zip(ratings['user_id_int'], ratings['product_id_int']))\n",
    "\n",
    "        num_negatives = 4\n",
    "        for u, i in user_item_set:\n",
    "            users.append(u)\n",
    "            items.append(i)\n",
    "            labels.append(1)\n",
    "            for _ in range(num_negatives):\n",
    "                negative_item = np.random.choice(all_product_ids)\n",
    "                while (u, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_product_ids)\n",
    "                users.append(u)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0)\n",
    "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9a572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be in seperate file\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NCF(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_users, num_items, ratings, all_product_ids):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
    "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "        self.ratings = ratings\n",
    "        self.all_product_ids = all_product_ids\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        \n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predicted_labels = self(user_input, item_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(RatingsTrainDataset(self.ratings, self.all_product_ids),\n",
    "                          batch_size=512, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671420e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file should be in seperate file\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import pickle\n",
    "\n",
    "def build_filtering_model(product_data_path, rating_data_path, company_id):\n",
    "    \n",
    "    # loading data\n",
    "    product_data = pd.read_csv(product_data_path)\n",
    "    rating_data = pd.read_csv(rating_data_path)\n",
    "    \n",
    "    # preprocessing data\n",
    "    encoder1 = LabelEncoder()\n",
    "    encoder2 = LabelEncoder()\n",
    "    rating_data['user_id_int'] = encoder1.fit_transform(rating_data['user_id'])\n",
    "    product_data['product_id_int'] = encoder2.fit_transform(product_data['product_id'])\n",
    "    user_mapping = {}\n",
    "    reverse_user_mapping = {}\n",
    "    for (original, mapped) in zip(rating_data['user_id'].unique() ,rating_data['user_id_int'].unique()):\n",
    "        user_mapping[original] = mapped\n",
    "        reverse_user_mapping[mapped] = original\n",
    "    product_mapping = {}\n",
    "    reverse_product_mapping = {}\n",
    "    for (original, mapped) in zip(product_data['product_id'].unique() ,product_data['product_id_int'].unique()):\n",
    "        product_mapping[original] = mapped\n",
    "        reverse_product_mapping[mapped] = original\n",
    "    rating_data['product_id_int'] = rating_data[\"product_id\"].apply(lambda x: product_mapping.get(x))\n",
    "    \n",
    "    print('preprocessing completed')\n",
    "\n",
    "    # Building Content Based Filtering Model\n",
    "    product_data['full_description'] = product_data['title'] + \" \" + product_data['description']\n",
    "    product_data['full_description'] = product_data['full_description'].fillna('')\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df = 0, stop_words='english')\n",
    "    tfidf_matrix = tf.fit_transform(product_data['full_description'])\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    product_data = product_data.reset_index()\n",
    "    indices = pd.Series(product_data.index, index = product_data['product_id_int'])\n",
    "    \n",
    "    print('content based modeling completed')\n",
    "    \n",
    "    # Building Collaborative Filtering Model\n",
    "    train_ratings = rating_data[['user_id_int', 'product_id_int', 'rating']]\n",
    "    all_product_ids = rating_data['product_id_int'].unique()\n",
    "    users, items, labels = [], [], []\n",
    "    user_item_set = set(zip(train_ratings['user_id_int'], train_ratings['product_id_int']))\n",
    "    num_negatives = 4\n",
    "    for (u, i) in user_item_set:\n",
    "        users.append(u)\n",
    "        items.append(i)\n",
    "        labels.append(1)\n",
    "        for _ in range(num_negatives):\n",
    "            negative_item = np.random.choice(all_product_ids) \n",
    "            while (u, negative_item) in user_item_set:\n",
    "                negative_item = np.random.choice(all_product_ids)\n",
    "            users.append(u)\n",
    "            items.append(negative_item)\n",
    "            labels.append(0)\n",
    "\n",
    "    num_users = rating_data['user_id_int'].max()+1\n",
    "    num_items = rating_data['product_id_int'].max()+1\n",
    "    all_product_ids = rating_data['product_id_int'].unique()\n",
    "\n",
    "    model = NCF(num_users, num_items, train_ratings, all_product_ids)\n",
    "    trainer = pl.Trainer(max_epochs=5,logger=True, accelerator='auto')\n",
    "    trainer.fit(model)\n",
    "    user_interacted_items = rating_data.groupby('user_id_int')['product_id_int'].apply(list).to_dict()\n",
    "    print('collaborative filtering modeling completed')\n",
    "    \n",
    "    # store mapping data\n",
    "    if not os.path.exists(company_id):\n",
    "        os.mkdir(company_id)\n",
    "        \n",
    "    with open(f'{company_id}/user_mapping.pkl', 'wb') as file:\n",
    "        pickle.dump(user_mapping, file)\n",
    "    print(f'Created {company_id}/user_mapping.pkl')\n",
    "        \n",
    "    with open(f'{company_id}/reverse_user_mapping.pkl', 'wb') as file:\n",
    "        pickle.dump(reverse_user_mapping, file)\n",
    "    print(f'Created {company_id}/reverse_user_mapping.pkl')\n",
    "        \n",
    "    with open(f'{company_id}/product_mapping.pkl', 'wb') as file:\n",
    "        pickle.dump(product_mapping, file)\n",
    "    print(f'Created {company_id}/product_mapping.pkl')\n",
    "        \n",
    "    with open(f'{company_id}/reverse_product_mapping.pkl', 'wb') as file:\n",
    "        pickle.dump(reverse_product_mapping, file)\n",
    "    print(f'Created {company_id}/reverse_product_mapping.pkl')\n",
    "        \n",
    "    # store content based filtering model\n",
    "    with open(f'{company_id}/content_based_model.pkl', 'wb') as file:\n",
    "        pickle.dump({'cosine_sim': cosine_sim, 'series': indices}, file)\n",
    "    print(f'Created {company_id}/content_based_model.pkl')\n",
    "        \n",
    "    # store collaborative filtering model\n",
    "    with open(f'{company_id}/collaborative_filtering_model.pkl', 'wb') as file:\n",
    "        pickle.dump({'product_ids': all_product_ids, 'model': model, 'interacted_items': user_interacted_items}, file)\n",
    "    print(f'Created {company_id}/collaborative_filtering_model.pkl')\n",
    "    \n",
    "    \n",
    "    print('Models Created Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb73972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing completed\n",
      "content based modeling completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/suyashnehete/Suyash/PCCOE/final year/project/Final/lightning_logs\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | user_embedding | Embedding | 339 K \n",
      "1 | item_embedding | Embedding | 440 K \n",
      "2 | fc1            | Linear    | 1.1 K \n",
      "3 | fc2            | Linear    | 2.1 K \n",
      "4 | output         | Linear    | 33    \n",
      "---------------------------------------------\n",
      "783 K     Trainable params\n",
      "0         Non-trainable params\n",
      "783 K     Total params\n",
      "3.136     Total estimated model params size (MB)\n",
      "/Users/suyashnehete/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b132846e26446bbff5db9560481da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collaborative filtering modeling completed\n",
      "Created xjkgkjshl/user_mapping.pkl\n",
      "Created xjkgkjshl/reverse_user_mapping.pkl\n",
      "Created xjkgkjshl/product_mapping.pkl\n",
      "Created xjkgkjshl/reverse_product_mapping.pkl\n",
      "Created xjkgkjshl/content_based_model.pkl\n",
      "Created xjkgkjshl/collaborative_filtering_model.pkl\n",
      "Models Created Successfully\n"
     ]
    }
   ],
   "source": [
    "# call this for building model\n",
    "build_filtering_model('products.csv', 'ratings.csv', 'xjkgkjshl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3aa1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a65a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
